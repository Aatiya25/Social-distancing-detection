{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[Config file for Bird View loaded] \u001b[0m\n",
      "\u001b[92m Done : [ Config file loaded ] ...\u001b[0m\n",
      "\u001b[93m [ Loading the MODEL ... ]\u001b[0m\n",
      "\u001b[92mDone : [ Model loaded and initialized ] ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "COLOR_RED = (0, 0, 255)\n",
    "COLOR_GREEN = (0, 255, 0)\n",
    "COLOR_BLUE = (255, 0, 0)\n",
    "BIG_CIRCLE = 60\n",
    "SMALL_CIRCLE = 3\n",
    "\n",
    "# defining colors\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "\n",
    "# Bird View\n",
    "def compute_perspective_transform(corner_points,width,height,image):\n",
    "\n",
    "    # Create an array out of the 4 corner points\n",
    "    corner_points_array = np.float32(corner_points)\n",
    "    # Create an array with the parameters (the dimensions) required to build the matrix\n",
    "    img_params = np.float32([[0,0],[width,0],[0,height],[width,height]])\n",
    "    # Compute and return the transformation matrix\n",
    "    matrix = cv2.getPerspectiveTransform(corner_points_array,img_params) \n",
    "    img_transformed = cv2.warpPerspective(image,matrix,(width,height))\n",
    "    return matrix,img_transformed\n",
    "\n",
    "\n",
    "def compute_point_perspective_transformation(matrix,list_downoids):\n",
    "\n",
    "    # Compute the new coordinates of our points\n",
    "    list_points_to_detect = np.float32(list_downoids).reshape(-1, 1, 2)\n",
    "    transformed_points = cv2.perspectiveTransform(list_points_to_detect, matrix)\n",
    "    # Loop over the points and add them to the list that will be returned\n",
    "    transformed_points_list = list()\n",
    "    for i in range(0,transformed_points.shape[0]):\n",
    "        transformed_points_list.append([transformed_points[i][0][0],transformed_points[i][0][1]])\n",
    "    return transformed_points_list\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def get_human_box_detection(boxes,scores,classes,height,width):\n",
    "\n",
    "    array_boxes = list() # Create an empty list\n",
    "    for i in range(boxes.shape[1]):\n",
    "        # If the class of the detected object is 1 and the confidence of the prediction is > 0.6\n",
    "        if int(classes[i]) == 1 and scores[i] > 0.75:\n",
    "            # Multiply the X coordonnate by the height of the image and the Y coordonate by the width\n",
    "            # To transform the box value into pixel coordonate values.\n",
    "            box = [boxes[0,i,0],boxes[0,i,1],boxes[0,i,2],boxes[0,i,3]] * np.array([height, width, height, width])\n",
    "            # Add the results converted to int\n",
    "            array_boxes.append((int(box[0]),int(box[1]),int(box[2]),int(box[3])))\n",
    "    return array_boxes\n",
    "\n",
    "\n",
    "def get_centroids_and_groundpoints(array_boxes_detected):\n",
    "\n",
    "    array_centroids,array_groundpoints = [],[] # Initialize empty centroid and ground point lists \n",
    "    for index,box in enumerate(array_boxes_detected):\n",
    "        # Draw the bounding box \n",
    "        # Get the both important points\n",
    "        centroid,ground_point = get_points_from_box(box)\n",
    "        array_centroids.append(centroid)\n",
    "        array_groundpoints.append(centroid)\n",
    "    return array_centroids,array_groundpoints\n",
    "\n",
    "\n",
    "def get_points_from_box(box):\n",
    "\n",
    "    # Center of the box x = (x1+x2)/2 et y = (y1+y2)/2\n",
    "    center_x = int(((box[1]+box[3])/2))\n",
    "    center_y = int(((box[0]+box[2])/2))\n",
    "    # Coordiniate on the point at the bottom center of the box\n",
    "    center_y_ground = center_y + ((box[2] - box[0])/2)\n",
    "    return (center_x,center_y),(center_x,int(center_y_ground))\n",
    "\n",
    "\n",
    "def change_color_on_topview(pair):\n",
    "\n",
    "    cv2.circle(bird_view_img, (pair[0][0],pair[0][1]), BIG_CIRCLE, COLOR_RED, 2)\n",
    "    cv2.circle(bird_view_img, (pair[0][0],pair[0][1]), SMALL_CIRCLE, COLOR_RED, -1)\n",
    "    cv2.circle(bird_view_img, (pair[1][0],pair[1][1]), BIG_CIRCLE, COLOR_RED, 2)\n",
    "    cv2.circle(bird_view_img, (pair[1][0],pair[1][1]), SMALL_CIRCLE, COLOR_RED, -1)\n",
    "\n",
    "def draw_rectangle(corner_points):\n",
    "    # Draw rectangle box over the delimitation area\n",
    "    cv2.line(frame, (corner_points[0][0], corner_points[0][1]), (corner_points[1][0], corner_points[1][1]), COLOR_BLUE, thickness=1)\n",
    "    cv2.line(frame, (corner_points[1][0], corner_points[1][1]), (corner_points[3][0], corner_points[3][1]), COLOR_BLUE, thickness=1)\n",
    "    cv2.line(frame, (corner_points[0][0], corner_points[0][1]), (corner_points[2][0], corner_points[2][1]), COLOR_BLUE, thickness=1)\n",
    "    cv2.line(frame, (corner_points[3][0], corner_points[3][1]), (corner_points[2][0], corner_points[2][1]), COLOR_BLUE, thickness=1)\n",
    "\n",
    "# Top view configuration file\n",
    "print(bcolors.WARNING +\"[Config file for Bird View loaded] \"+ bcolors.ENDC)\n",
    "with open(\"/User/source/config_birdview.yml\", \"r\") as ymlfile:\n",
    "    cfg = yaml.load(ymlfile)\n",
    "width_og, height_og = 0,0\n",
    "corner_points = []\n",
    "for section in cfg:\n",
    "    corner_points.append(cfg[\"image_parameters\"][\"p1\"])\n",
    "    corner_points.append(cfg[\"image_parameters\"][\"p2\"])\n",
    "    corner_points.append(cfg[\"image_parameters\"][\"p3\"])\n",
    "    corner_points.append(cfg[\"image_parameters\"][\"p4\"])\n",
    "    width_og = int(cfg[\"image_parameters\"][\"width_og\"])\n",
    "    height_og = int(cfg[\"image_parameters\"][\"height_og\"])\n",
    "    img_path = cfg[\"image_parameters\"][\"img_path\"]\n",
    "    #size_frame = cfg[\"imageparameters\"][\"size_frame\"]\n",
    "print(bcolors.OKGREEN +\" Done : [ Config file loaded ] ...\"+bcolors.ENDC )\n",
    "\n",
    "#Model Selection\n",
    "class model:\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        \n",
    "        # Declare detection graph\n",
    "        self.detection_graph = tf.Graph()\n",
    "        # Load the model using the tensorflow graph\n",
    "        with self.detection_graph.as_default():\n",
    "            od_graph_def = tf.compat.v1.GraphDef()\n",
    "            with tf.io.gfile.GFile(model_path, 'rb') as file:\n",
    "                serialized_graph = file.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        # Create a session from the detection graph\n",
    "        self.sess = tf.compat.v1.Session(graph=self.detection_graph)\n",
    "\n",
    "    def predict(self,img):\n",
    "        \n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        img_exp = np.expand_dims(img, axis=0)\n",
    "        # Pass the inputs and outputs to the session to get the results \n",
    "        (boxes, scores, classes) = self.sess.run([self.detection_graph.get_tensor_by_name('detection_boxes:0'), self.detection_graph.get_tensor_by_name('detection_scores:0'), self.detection_graph.get_tensor_by_name('detection_classes:0')],feed_dict={self.detection_graph.get_tensor_by_name('image_tensor:0'): img_exp})\n",
    "        return (boxes, scores, classes)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_path='/Users/source/faster_rcnn_inception_v2_coco_2018_01_28'\n",
    "print(bcolors.WARNING + \" [ Loading the MODEL ... ]\"+bcolors.ENDC)\n",
    "model = model(model_path)\n",
    "print(bcolors.OKGREEN +\"Done : [ Model loaded and initialized ] ...\"+bcolors.ENDC)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Video Path\n",
    "video_path='/Users/source/PETS2009.avi'\n",
    "\n",
    "#Minimum Distance\n",
    "distance_minimum = \"110\"\n",
    "\n",
    "def CallBackFunc(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        print(\"Left button of the mouse is clicked - position (\", x, \", \",y, \")\")\n",
    "        list_points.append([x,y])\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        print(\"Right button of the mouse is clicked - position (\", x, \", \", y, \")\")\n",
    "        list_points.append([x,y])\n",
    "\n",
    "# Transform Matrix\n",
    "# Compute  transformation matrix from the original frame\n",
    "matrix,imgOutput = compute_perspective_transform(corner_points,width_og,height_og,cv2.imread(img_path))\n",
    "height,width,_ = imgOutput.shape\n",
    "blank_image = np.zeros((height,width,3), np.uint8)\n",
    "height = blank_image.shape[0]\n",
    "width = blank_image.shape[1] \n",
    "dim = (width, height)\n",
    "\n",
    "\n",
    "\n",
    "## VIDEO Start##\n",
    "\n",
    "vs = cv2.VideoCapture(video_path)\n",
    "output_video_1,output_video_2 = None,None\n",
    "# Loop until the end of the video stream\n",
    "while True:\t\n",
    "    # Load the image of the ground and resize it to the correct size\n",
    "    img = cv2.imread(\"/Users/source/view_1.png\")\n",
    "    bird_view_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    # Load the frame\n",
    "    (frame_exists, frame) = vs.read()\n",
    "    # Test if it has reached the end of the video\n",
    "    if not frame_exists:\n",
    "        break\n",
    "    else:\n",
    "        # Resize the image to the correct size\n",
    "        frame = imutils.resize(frame, width=int(900))\n",
    "\n",
    "        # Make the predictions for this frame\n",
    "        (boxes, scores, classes) =  model.predict(frame)\n",
    "\n",
    "        # Get the human detected in the frame and return the 2 points to build the bounding box  \n",
    "        array_boxes_detected = get_human_box_detection(boxes,scores[0].tolist(),classes[0].tolist(),frame.shape[0],frame.shape[1])\n",
    "\n",
    "        # Both of our lists that will contain the centroïds coordonates and the ground points\n",
    "        array_centroids,array_groundpoints = get_centroids_and_groundpoints(array_boxes_detected)\n",
    "\n",
    "        # Use the transform matrix to get the transformed coordonates\n",
    "        transformed_downoids = compute_point_perspective_transformation(matrix,array_groundpoints)\n",
    "        # Show every point on the top view image \n",
    "        #for point in transformed_downoids:\n",
    "            #x,y = point\n",
    "            #cv2.circle(bird_view_img, (x,y), BIG_CIRCLE, COLOR_GREEN, 2)\n",
    "            #cv2.circle(bird_view_img, (x,y), SMALL_CIRCLE, COLOR_GREEN, -1)\n",
    "\n",
    "        # Check if 2 or more people have been detected (otherwise no need to detect)\n",
    "        if len(transformed_downoids) >= 2:\n",
    "            for index,downoid in enumerate(transformed_downoids):\n",
    "                if not (downoid[0] > width or downoid[0] < 0 or downoid[1] > height+200 or downoid[1] < 0 ):\n",
    "                    cv2.rectangle(frame,(array_boxes_detected[index][1],array_boxes_detected[index][0]),(array_boxes_detected[index][3],array_boxes_detected[index][2]),COLOR_GREEN,2)\n",
    "\n",
    "            # Iterate over every possible 2 by 2 between the points combinations \n",
    "            list_indexes = list(itertools.combinations(range(len(transformed_downoids)), 2))\n",
    "            for i,pair in enumerate(itertools.combinations(transformed_downoids, r=2)):\n",
    "                # Check if the distance between each combination of points is less than the minimum distance chosen\n",
    "                if math.sqrt( (pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2 ) < int(distance_minimum):\n",
    "                    # Change the colors of the points that are too close from each other to red\n",
    "                    if not (pair[0][0] > width or pair[0][0] < 0 or pair[0][1] > height+200  or pair[0][1] < 0 or pair[1][0] > width or pair[1][0] < 0 or pair[1][1] > height+200  or pair[1][1] < 0):\n",
    "                        change_color_on_topview(pair)\n",
    "                        # Get the equivalent indexes of these points in the original frame and change the color to red\n",
    "                        index_pt1 = list_indexes[i][0]\n",
    "                        index_pt2 = list_indexes[i][1]\n",
    "                        cv2.rectangle(frame,(array_boxes_detected[index_pt1][1],array_boxes_detected[index_pt1][0]),(array_boxes_detected[index_pt1][3],array_boxes_detected[index_pt1][2]),COLOR_RED,2)\n",
    "                        cv2.rectangle(frame,(array_boxes_detected[index_pt2][1],array_boxes_detected[index_pt2][0]),(array_boxes_detected[index_pt2][3],array_boxes_detected[index_pt2][2]),COLOR_RED,2)\n",
    "\n",
    "\n",
    "    # Draw the green rectangle to delimitate the detection zone\n",
    "    draw_rectangle(corner_points)\n",
    "    # Show both images\t\n",
    "    cv2.imshow(\"Bird view\", bird_view_img)\n",
    "    #cv2.imshow(\"Detection\", frame)\n",
    "\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Write the both outputs video to a local folders\n",
    "    if output_video_1 is None and output_video_2 is None:\n",
    "        fourcc1 = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        output_video_1 = cv2.VideoWriter(\"/output/video.avi\", fourcc1, 25,(frame.shape[1], frame.shape[0]), True)\n",
    "        fourcc2 = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        output_video_2 = cv2.VideoWriter(\"/output/bird_view.avi\", fourcc2, 25,(bird_view_img.shape[1], bird_view_img.shape[0]), True)\n",
    "    elif output_video_1 is not None and output_video_2 is not None:\n",
    "        output_video_1.write(frame)\n",
    "        output_video_2.write(bird_view_img)\n",
    "\n",
    "    # Break the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
